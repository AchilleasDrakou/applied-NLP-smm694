{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "# ―Sentiment, affect, and connotation\n",
    "\n",
    "<img src=\"images/_1.jpeg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why is sentiment analysis (SA) so popular?\n",
    "\n",
    "+ Extracting audiences' sentiment is a general-class task:\n",
    "  * a review of a movie, book, or product on the web expresses the author’s \n",
    "    sentiment toward the product\n",
    "  * an editorial or political text expresses sentiment toward a candidate or \n",
    "    political action\n",
    "+ Online behavior offers germane conditions to SA: \n",
    "  * social media exhibit sharp community structures (e.g., \n",
    "    [Schmidt et al. 2017][1]) \n",
    "  * hence, opposing views are fostered (e.g., [Ball et al. 2018][2])\n",
    "  * bots increase exposure to negative and inflammatory content \n",
    "    ([Stella, Ferrara, & Domenico 2018][3])\n",
    "  * the marginal costs of engaging in social sanctioning $\\rightarrow$ 0:\n",
    "    - online firestorms emerge frequently ([Rost, Stahel & Frey, 2016][4])\n",
    "  * ... and the costs of being sanctioned $\\rightarrow$ 0\n",
    "\n",
    "  [1]: https://www.pnas.org/content/114/12/3035.short\n",
    "  [2]: pnas.org/content/115/37/9216?mod=article_inline\n",
    "  [3]: https://www.pnas.org/content/115/49/12435.short\n",
    "  [4]: https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0155923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP and classification tasks\n",
    "\n",
    "The simplest version of sentiment analysis is a binary classification task ―\n",
    "the words of the review provide excellent cues (Martin & Jurafsky, 2019).\n",
    "\n",
    "Consider, for example, the following phrases extracted from positive and negative \n",
    "reviews of movies and restaurants:\n",
    "\n",
    "```{bash}\n",
    "...awesome caramel sauce and sweet toasty almonds. I love this place! \n",
    "...awful pizza and ridiculously overpriced...\n",
    "```\n",
    "\n",
    "Counting an offering's promoters and/or detractors implies classifying reviews\n",
    "into dsicrete classes, such as 'positive' and 'negative'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Classifiers in a nutshell\n",
    "\n",
    "The task of supervised classification is to take an input $x$ and a fixed\n",
    "set of output classes $Y = y_{1}, y_{2}, \\ldots, y_{M}$ and return a predicted \n",
    "class $y \\in Y$.\n",
    "\n",
    "There are two broad familiies of classifiers:\n",
    "+ **Generative classifiers**, like naive Bayes, build a model of how a class   \n",
    "  could generate some input data:\n",
    "  * given an observation, they return the class most likely to have generated\n",
    "    the observation\n",
    "+ **Discriminative classifiers**, like logistic regression, learn what features \n",
    "  from the input are most useful to discriminate between the different possible \n",
    "  classes\n",
    "\n",
    "Naive Bayes has been widely applied to SA ― let's have a closer look..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Naive Bayes Classifier (NBC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ So called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact.\n",
    "+ The intuition of the classifier is shown in the below-displayed figure:\n",
    "  * text documents are represented as bag-of-words:\n",
    "    - the position of words doesn't matter\n",
    "    - what matters is the frequency of words in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_2.png\" width=\"100%\">\n",
    "\n",
    "Source is Jurafsky & Martin, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Intuition of the Naive Bayes Classifier\n",
    "\n",
    "For a document $d$, out of all classes $c \\in C$, the classifier returns the \n",
    "class $\\hat c$ which has the maximum posterior probability given the document:\n",
    "\n",
    "\\begin{equation}\\label{eq:}\n",
    "\\hat c = argmaxP*(c|d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference\n",
    "\n",
    "This idea of Bayesian inference has been known since the work of Bayes* (1763),\n",
    "and was first applied to text classification by Mosteller and Wallace (1964). \n",
    "\n",
    "The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. \n",
    "1 into other probabilities that have some useful properties. Bayes’ rule is \n",
    "presented in three other probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "P(x|y) = \\frac{P(y|x)P(x)}{P(y)}\n",
    "\\end{equation}\n",
    "\n",
    "\\* He's interred in Bunhill Fields Cemetery\n",
    "\n",
    "<img src=\"images/_3.jpeg\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "Eq. [2] can be substituted into Eq. [1] to get Eq. [3]:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat c = argmax \\frac{P(d|c)P(c)}{P(d)}\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$\n",
    "\n",
    "As $P(d)$ doesn't change for each class, Eq. [3] can be dispensed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat c = argmax P(d|c)P(c)\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "Eq. [4] contains two probabilities:\n",
    "\n",
    "+ $P(c)$ is the prior probability of the class $c$\n",
    "+ $P(d|c)$ is the likelihood of the document, which can also be expressed as:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\hat c = argmaxP(f_{1}, f_{2}, ..., f_{n}|c)P(c)\n",
    "  \\end{equation}\n",
    "  \n",
    "In practice, Eq. [5] is just too costly/impossible to estimate:\n",
    "+ estimating the probability of every possible combination of features would require:\n",
    "  * huge numbers of parameters\n",
    "  * impossibly large training sets\n",
    "+ NBCs therefore make two simplifying assumptions:\n",
    "  * 'bag of words' assumption ― the order of words doesn't matter\n",
    "    - the vector of features $F$ encodes word identities not positions\n",
    "  * 'naive Bayes assumption': the probabilities $P(f_{i}|c)$ are independent\n",
    "    given the class $c$\n",
    "    \n",
    "    $P(f_{1}, f_{2}, ..., f_{n}) = P(f_{1}|c) \\cdot P(f_{2}|c) \\cdot ... \n",
    "      \\cdot P(f_{n}|c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "The final equation for the class chosen by a NBC is thus:\n",
    "\n",
    "\\begin{equation}\n",
    "c_{NB} = argmaxP(c) \\Pi P(f|c)\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$ and $f \\in F$\n",
    "\n",
    "As Jurafsky and Martin (2019) note, Naive Bayes calculations, like calculations for \n",
    "language modeling, are done in log space, to avoid underflow and increase speed. Hence,\n",
    "Eq. [6] can dispensed as follows [7]:\n",
    "\n",
    "\\begin{equation}\n",
    "c_{NB} = argmax log P(c) + \\sum log P(w_{i}|c)\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$ and $i \\in V$, where $V$ is the dictionary of the corpus of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training the NBC ― prior probabilities\n",
    "\n",
    "For the document prior $P(c)$ we ask what percentage of the documents in our training \n",
    "set are in each class c.\n",
    "\n",
    "Let Nc be the number of documents in our training data with class $c$ and $N_{doc}$ be \n",
    "the total number of documents:\n",
    "\n",
    "$\\hat P(c) = \\frac{N_{c}}{N_{doc}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training the NBC ― conditional distributions of features\n",
    "\n",
    "To learn the probability $P(f_{i}|c)$, we'll assume a feature is just the existence of a \n",
    "word in the document's bag of words (Jurafsky & Martin, 2019).\n",
    "\n",
    "$P(w_{i}|c)$ is the fraction of times the word $w_{i}$ appears among all wods in all \n",
    "documents of class $c$:\n",
    "+ we first concatenate all documents with class $c$\n",
    "+ then, we use the frequency of $w_{i}$ in this concatenated document to give a maximum\n",
    "  likelihood estimate of the probability:\n",
    "  \n",
    "  $\\hat P(w_{i}|c) = \\frac{|w_{i}, c|}{\\sum_{w \\in V} |w, c|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training the NBC ― caveats\n",
    "\n",
    "**Problem:** since NBC multiplies all the features likelihood $(w_{i}|c)$, \n",
    "zero-probabilites in any  word included in a test set document will cause the \n",
    "probability of the class to be zero!\n",
    "\n",
    "Non mutually exclusive **solutions**:\n",
    "\n",
    "+ add-one (Laplace) smoothing\n",
    "+ torough NLP pipeline dealing with rare or oov words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example\n",
    "\n",
    "+ Setup:\n",
    "  * sentiment analysis domain with the two classes:\n",
    "    - positive (+) and \n",
    "    - negative (-)\n",
    "+ Data:\n",
    "  * training and test documents simplified from actual movie reviews\n",
    "  \n",
    "| Set      | Class | Documents                             |\n",
    "|----------|-------|---------------------------------------|\n",
    "| Training | -     | just plain boring                     |\n",
    "|          | -     | entirely predictable and lacks energy | \n",
    "|          | -     | no surprise and very few laughs       |\n",
    "|          | +     | very powerful                         |\n",
    "|          | +     | the most fun film of the summer       |\n",
    "| Test     | ?     | predictable with no fun               |\n",
    "\n",
    "Source is Jurafsky and Martin (2019, page: 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: computing priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| Set      | Class | Documents                             |\n",
    "|----------|-------|---------------------------------------|\n",
    "| Training | -     | just plain boring                     |\n",
    "|          | -     | entirely predictable and lacks energy | \n",
    "|          | -     | no surprise and very few laughs       |\n",
    "|          | +     | very powerful                         |\n",
    "|          | +     | the most fun film of the summer       |\n",
    "| Test     | ?     | predictable with no fun               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(-) = \\frac{3}{5}$\n",
    "\n",
    "$P(+) = \\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: computing word likelihood\n",
    "\n",
    "Design:\n",
    "\n",
    "+ oov words are filtered out\n",
    "  * we don't consider 'w'\n",
    "+ we use 'add-one' smoothing\n",
    "  * $\\hat P(w_{i}|c) = \\frac{|w_{i}, c|}{\\sum_{w \\in V} |w, c| + 1}$\n",
    "\n",
    "| Word        | $c = -$              | $c = +$            |\n",
    "| ----------- | -------------------- | -------------------|\n",
    "| predictable |  (1 + 1) / (14 + 20) | (0 + 1) / (9 + 20) |\n",
    "| no          |  (1 + 1) / (14 + 20) | (0 + 1) / (9 + 20) |\n",
    "| fun         |  (0 + 1) / (14 + 20) | (1 + 1) / (9 + 20) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: estimating doc-to-class affiliations \n",
    "\n",
    "$P(-)P(s|-) = \\frac{3}{5} \\times \\frac{2 \\times 2 \\times 1}{34^{3}} = \n",
    "6.1 \\times 10^{-5}$\n",
    "\n",
    "$P(+)P(s|+) = \\frac{2}{5} \\times \\frac{1 \\times 1 \\times 2}{29^{3}} = \n",
    "3.2 \\times 10^{-5}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Over and beyond SA...\n",
    "\n",
    "SA focuses on a very specific portion of a braoder gamut containing affective states.\n",
    "\n",
    "Prior literature (Scherer, 2000) suggests the existence of some families of affectives states that can be investigated with NLP theories and tools:\n",
    "\n",
    "+ **emotion:** relatively brief episode of response to the evaluation of an \n",
    "  external or internal event as being of major significance (angry, sad, \n",
    "  joyful, fearful, ashamed, proud, elated, desperate)\n",
    "+ **mood:** diffuse affect state, most pronounced as change in subjective feeling, \n",
    "  of low intensity but relatively long duration, often without apparent cause\n",
    "  (cheerful, gloomy, supportive, contemptous, friendly)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Over and beyond SA...(cont'd)\n",
    "\n",
    "+ **interpersonal stance:** affective stance taken toward another person in a \n",
    "  specific interaction, coloring the interpersonal exchange in a situation\n",
    "  (distant, cold, warm, supportive, contemptous, friendly)\n",
    "+ **attitudes:** relatively enduring, affectively colored beliefs, preferences, and\n",
    "  predispositions towards objects or persons (liking, loving, hating, valuing\n",
    "  desiring)\n",
    "+ **personality traits:** emotionally laden, stable personality dispositions and\n",
    "  behavior tendencies, typical for a person (nervous, anxious, reckless,   \n",
    "  hostile, jealous)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Affect lexicons\n",
    "\n",
    "+ the study of affect states via NLP draws upon affect lexicons\n",
    "+ the key premise of affect lexicons is that words have affect \n",
    "  meanings ― i.e., they have connotations\n",
    "+ affect lexicons can be distinguished along several dimensions:\n",
    "  * polar lexicons Vs. cluster-based lexicons\n",
    "  * human-annotated/supervised lexicons Vs. semi-supervised lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example of polar, human-annotated affect lexicon\n",
    "\n",
    "<img src=\"images/_5.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example of cluster-based, human-annotated affect lexicon\n",
    "\n",
    "<img src=\"images/_6.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Semi-supervised induction of affect lexicons\n",
    "\n",
    "There are two popular alternatives to create affect lexicons using a \n",
    "semi-supervised approach:\n",
    "    \n",
    "+ semantic axis methods\n",
    "+ label propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Semantic Axis Method (SAM)\n",
    "\n",
    "The semantic axis method builds on four steps:\n",
    "\n",
    "1. pole-by-pole (or aspect-by-aspect), fixing seed words (hand-curated task)\n",
    "2. getting the embedding for each word\n",
    "3. getting the embedding for each pole (or aspect)\n",
    "4. getting the semantic axis\n",
    "5. appreciating the position of words in the semantic axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SAM: Step 1\n",
    "\n",
    "Based on target affect states or poles (e.g., 'good' and 'bad'), a list\n",
    "of seed words is compiled.\n",
    "\n",
    "**Caveat**: word meanings are relational in nature:\n",
    "\n",
    "+ meanings change based on the linguistic context for a lexical item\n",
    "+ meanings change across space and time\n",
    "+ thus, make sure your seed words 'make sense' given the dataset at hand\n",
    "\n",
    "<img src=\"images/_7.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SAM: Step 2 ― getting word embeddings\n",
    "\n",
    "Each target word has to be associated with a vector. Mainly, there are two\n",
    "alternatives:\n",
    "\n",
    "+ using off-the-shelf embeddings \n",
    "+ building your own embedding (desirable for idiosyncratic text corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SAM: Step 3 ― getting pole (aspect) embeddings\n",
    "\n",
    "Once word vectors are available, we need to create embeddings for the poles (aspects).\n",
    "\n",
    "This can be achieved by taking the centroid of the word vectors associated with\n",
    "each pole (or state).\n",
    "\n",
    "Let's consider two sets of word vectors:\n",
    "\n",
    "$S^{+} = \\{E{w_{1}^{+}}, E{w_{2}^{+}}, ..., E{w_{n}^{+}}\\}$\n",
    "\n",
    "and\n",
    "\n",
    "$S^{-} = \\{E{w_{1}^{-}}, E{w_{2}^{-}}, ..., E{w_{n}^{-}}\\}$\n",
    "\n",
    "where $S^{+}$ isf the embedding of, say, positive seed words,\n",
    "and $S^{-}$ is the embedding of negative words.\n",
    "\n",
    "The embeddings of the positive and the negative poles are, respectively:\n",
    "\n",
    "$V^{+} = \\frac{1}{n} \\sum_{i = 1}^{n} E(w_{i}^{+})$\n",
    "\n",
    "$V^{-} = \\frac{1}{m} \\sum_{i = 1}^{m} E(w_{i}^{-})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SAM: Step 4 ― creating a semantic axis\n",
    "\n",
    "The semantic axis can be computed by subctracting the vectors of the poles \n",
    "(aspects) as follows:\n",
    "\n",
    "$V_{axis} = V^{+} - V^{-}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SAM: Step 5 ― locating word positions along the semantic axis\n",
    "\n",
    "The position of a target word $w$ along the semantic axis can be computed\n",
    "(for example) as the cosine similarity of $w$ and $V_{axis}$:\n",
    "\n",
    "$score(w) = \\frac {E(w) \\cdot V_{axis}}{||E(w)|| ||V_{axis}||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Label propagation\n",
    "\n",
    "<img src=\"images/_8.png\" width=\"80%\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
