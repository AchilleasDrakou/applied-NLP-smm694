{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "\n",
    "# ―Sentiment, affect, and connotation\n",
    "\n",
    "<img src=\"images/_1.jpeg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is sentiment analysis (SA) so popular?\n",
    "\n",
    "+ Extracting audiences' sentiment is a general-class task:\n",
    "  * a review of a movie, book, or product on the web expresses the author’s \n",
    "    sentiment toward the product\n",
    "  * an editorial or political text expresses sentiment toward a candidate or \n",
    "    political action\n",
    "+ Online behavior offers germane conditions to SA: \n",
    "  * social media exhibit sharp community structures (e.g., \n",
    "    [Schmidt et al. 2017][1]) \n",
    "  * hence, opposing views are fostered (e.g., [Ball et al. 2018][2])\n",
    "  * bots increase exposure to negative and inflammatory content \n",
    "    ([Stella, Ferrara, & Domenico 2018][3])\n",
    "  * the marginal costs of engaging in social sanctioning ~ 0:\n",
    "    - online firestorms emerge frequently ([Rost, Stahel & Frey, 2016][4])\n",
    "  * ... and the costs of being sanctioned ~ 0\n",
    "\n",
    "  [1]: https://www.pnas.org/content/114/12/3035.short\n",
    "  [2]: pnas.org/content/115/37/9216?mod=article_inline\n",
    "  [3]: https://www.pnas.org/content/115/49/12435.short\n",
    "  [4]: https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0155923"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and classification tasks\n",
    "\n",
    "The simplest version of sentiment analysis is a binary classification task, and\n",
    "the words of the review provide excellent cues (Martin & Jurafsky, 2019).\n",
    "\n",
    "Consider, for example, the following phrases extracted from positive and negative \n",
    "reviews of movies and restaurants:\n",
    "\n",
    "```{bash}\n",
    "...awesome caramel sauce and sweet toasty almonds. I love this place! \n",
    "...awful pizza and ridiculously overpriced...\n",
    "```\n",
    "\n",
    "Counting an offering's promoters and/or detractors implies classifying reviews\n",
    "into dsicrete classes, such as 'positive' and 'negative'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers in a nutshell\n",
    "\n",
    "The task of supervised classification is to take an input $x$ and a fixed\n",
    "set of output classes $Y = y_{1}, y_{2}, \\ldots, y_{M}$ and return a predicted class $y \\in Y$.\n",
    "\n",
    "There are two broad familiies of classifiers:\n",
    "+ **Generative classifiers**, like naive Bayes, build a model of how a class   \n",
    "  could generate some input data:\n",
    "  * given an observation, they return the class most likely to have generated   \n",
    "    the observation\n",
    "+ **Discriminative classifiers**, like logistic regression, learn what features   from the input are most useful to discriminate between the different possible   classes\n",
    "\n",
    "Naive Bayes has been widely applied to SA ― let's have a closer look..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "+ So called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact.\n",
    "+ The intuition of the classifier is shown in the below-displayed figure:\n",
    "  * text documents are represented as bag-of-words:\n",
    "    - the position of words doesn't matter\n",
    "    - what matters is the frequency of words in the document\n",
    "    \n",
    "<img src=\"images/_2.png\" width=\"50%\">\n",
    "\n",
    "Source is Jurafsky & Martin, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition of the Naive Bayes Classifier\n",
    "\n",
    "For a document $d$, out of all classes $c \\in C$, the classifier returns the \n",
    "class $\\hat c$ which has the maximum posterior probability given the document:\n",
    "\n",
    "$\\hat c = argmaxP*(c|d) \\quad \\quad \\quad \\quad [1]$\n",
    "\n",
    "with $c \\in C$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference\n",
    "\n",
    "This idea of Bayesian inference has been known since the work of Bayes* (1763),\n",
    "and was first applied to text classification by Mosteller and Wallace (1964). \n",
    "\n",
    "The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. \n",
    "1 into other probabilities that have some useful properties. Bayes’ rule is \n",
    "presented in three other probabilities:\n",
    "\n",
    "$P(x|y) = \\frac{P(y|x)P(x)}{P(y)} \\quad \\quad \\quad \\quad [2]$\n",
    "\n",
    "\\* He's interred in Bunhill Fields Cemetery\n",
    "\n",
    "<img src=\"images/_3.jpeg\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "Eq. [2] can be substituted into Eq. [1] to get Eq. [3]:\n",
    "\n",
    "$\\hat c = argmax \\frac{P(d|c)P(c)}{P(d)} \\quad \\quad \\quad [3]$\n",
    "\n",
    "with $c \\in C$\n",
    "\n",
    "As $P(d)$ doesn't change for each class, Eq. [3] can be dispensed as follows:\n",
    "\n",
    "$\\hat c = argmax P(d|c)P(c) \\quad \\quad [4]$\n",
    "\n",
    "with $c \\in C$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "Eq. [4] contains two probabilities:\n",
    "\n",
    "+ $P(c)$ is the prior probability of the class $c$\n",
    "+ $P(d|c)$ is the likelihood of the document, which can also be expressed as:\n",
    "  * $\\hat c = argmaxP(f_{1}, f_{2}, ..., f_{n}|c)P(c) \\quad \\quad \\quad [5]$\n",
    "  \n",
    "In practice, Eq. [5] is just too costly/impossible to estimate:\n",
    "+ estimating the probability of every possible combination of features would require:\n",
    "  * huge numbers of parameters\n",
    "  * impossibly large training sets\n",
    "+ NBCs therefore make two simplifying assumptions:\n",
    "  * 'bag of words' assumption ― the order of words doesn't matter\n",
    "    - the vector of features $F$ encodes word identities not positions\n",
    "  * 'naive Bayes assumption': the probabilities $P(f_{i}|c)$ are independent\n",
    "    given the class $c$\n",
    "    \n",
    "    $P(f_{1}, f_{2}, ..., f_{n}) = P(f_{1}|c) \\cdot P(f_{2}|c) \\cdot ... \n",
    "      \\cdot P(f_{n}|c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "The final equation for the class chosen by a NBC is thus:\n",
    "\n",
    "$c_{NB} = argmaxP(c) \\Pi P(f|c) \\quad \\quad \\quad [6]$\n",
    "\n",
    "with $c \\in C$ and $f \\in F$\n",
    "\n",
    "As Jurafsky and Martin (2019) note, Naive Bayes calculations, like calculations for \n",
    "language modeling, are done in log space, to avoid underflow and increase speed. Hence,\n",
    "Eq. [6] can dispensed as follows [7]:\n",
    "\n",
    "$c_{NB} = argmax log P(c) + \\sum log P(w_{i}|c)$\n",
    "\n",
    "with $c \\in C$ and $i \\in V$, where $V$ is the dictionary of the corpus of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the NBC ― prior probabilities\n",
    "\n",
    "For the document prior $P(c)$ we ask what percentage of the documents in our training \n",
    "set are in each class c.\n",
    "\n",
    "Let Nc be the number of documents in our training data with class $c$ and $N_{doc}$ be \n",
    "the total number of documents:\n",
    "\n",
    "$\\hat P(c) = \\frac{N_{c}}{N_{doc}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the NBC ― conditional distributions of features\n",
    "\n",
    "To learn the probability $P(f_{i}|c)$, we'll assume a feature is just the existence of a \n",
    "word in the document's bag of words (Jurafsky & Martin, 2019).\n",
    "\n",
    "$P(w_{i}|c)$ is the fraction of times the word $w_{i}$ appears among all wods in all \n",
    "documents of class $c$:\n",
    "+ we first concatenate all documents with class $c$\n",
    "+ then, we use the frequency of $w_{i}$ in this concatenated document to give a maximum\n",
    "  likelihood estimate of the probability:\n",
    "  \n",
    "  $\\hat P(w_{i}|c) = \\frac{|w_{i}, c|}{\\sum_{w \\in V} |w, c|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the NBC ― caveats\n",
    "\n",
    "**Problem:** since NBC multiplies all the features likelihood $(w_{i}|c)$, \n",
    "zero-probabilites in any  word included in a test set document will cause the \n",
    "probability of the class to be zero!\n",
    "\n",
    "Non mutually exclusive **solutions**:\n",
    "\n",
    "+ add-one (Laplace) smoothing\n",
    "+ torough NLP pipeline dealing with rare or oov words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "+ Setup:\n",
    "  * sentiment analysis domain with the two classes:\n",
    "    - positive (+) and \n",
    "    - negative (-)\n",
    "+ Data:\n",
    "  * training and test documents simplified from actual movie reviews\n",
    "  \n",
    "| Set      | Class | Documents                             |\n",
    "|----------|-------|---------------------------------------|\n",
    "| Training | -     | just plain boring                     |\n",
    "|          | -     | entirely predictable and lacks energy | \n",
    "|          | -     | no surprise and very few laughs       |\n",
    "|          | +     | very powerful                         |\n",
    "|          | +     | the most fun film of the summer       |\n",
    "| Test     | ?     | predictable with no fun               |\n",
    "\n",
    "Source is Jurafsky and Martin (2019, page: 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: computing priors\n",
    "\n",
    "$P(-) = \\frac{3}{5}$\n",
    "\n",
    "$P(+) = \\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: computing word likelihood\n",
    "\n",
    "Design:\n",
    "\n",
    "+ oov words are filtered out\n",
    "  * we don't consider 'w'\n",
    "+ we use 'add-one' smoothing\n",
    "  * $\\hat P(w_{i}|c) = \\frac{|w_{i}, c|}{\\sum_{w \\in V} |w, c| + 1}$\n",
    "\n",
    "| Word        | c = - | c = + |\n",
    "| ----------- | ----- | ------|\n",
    "| predictable |  (1 + 1) / (14 + 20) | (0 + 1) / (9 + 20) |\n",
    "| no          |  (1 + 1) / (14 + 20) | (0 + 1) / (9 + 20) |\n",
    "| fun         |  (0 + 1) / (14 + 20) | (1 + 1) / (9 + 20) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: estimating doc-to-class affiliations \n",
    "\n",
    "$P(-)P(s|-) = \\frac{3}{5} \\times \\frac{2 \\times 2 \\times 1}{34^{3}} = 6.1 \\times 10^{-5}$\n",
    "\n",
    "$P(+)P(s|+) = \\frac{2}{5} \\times \\frac{1 \\times 1 \\times 2}{29^{3}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
