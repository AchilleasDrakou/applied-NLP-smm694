Week 2 ― README
===============

<!-- vim-markdown-toc GFM -->

* [Overview](#overview)
* [Materials](#materials)
* [Assignments](#assignments)

<!-- vim-markdown-toc -->


Overview
========

In Week 2, the attention revolves around:

+ the representation of words and meanings 
+ the roles language models play in any NLP pipelines


| Week (date)     | Agenda                                                                 |
|-----------------|------------------------------------------------------------------------|
| **2 (27-05)**   | **Representing words and meanings**                                    |
|                 | ― words and meanings in linguistics                                    |
|                 | ― words and meanings in machines                                       |
|                 | ― from WordNet, through discrete symbols, to word vectors and word2vec |
|                 | **Language modeling**                                                  |
|                 | ― pre-DL: N-gram modeling                                              |
|                 | ― post-DL: neural nets and neural language models                      |
|                 | ― part-of-speech tagging                                               |
|                 | ― parsing                                                              |
|                 | ― named entity recognition                                             |
|                 | ― vectors                                                              |
|                 | **Webinar**                                                            |
|                 | ― Q&A session                                                          |
|                 | ― using WordNet with NLTK                                              |
|                 | ― loading a pre-trained model of language (spaCy, Stanza)              |
|                 | ― processing text through NLP pipelines (spaCy, Stanza)                |
|                 | ― leveraging word vectors (NumPy)                                      |


Materials
=========

Below are the materials at the center of week 1

+ readings: 
  - lecture notes: [`ln_2.ipynb`](week2/ln_2.ipynb)
  - journal articles:
      * [example of 'simple' sentiment analysis](https://srdas.github.io/Papers/chat_FINAL.pdf)
      * [example of topic modeling](https://www.amyzang.org/uploads/2/6/5/5/26555370/publication_huang_lehavy_zang_and_zheng_2018_ms.pdf)
      * [a context-aware model of the language ― BERT](https://arxiv.org/pdf/1810.04805.pdf?)
      * [statistical analysis of word vectors and embeddings](https://arxiv.org/pdf/1902.00496.pdf)
      * [example of 'nuanced' sentiment analysis](http://tinyurl.com/y722xzjg)
      * [example of information extraction paper](https://www.sciencedirect.com/science/article/pii/s187705091932071x)
+ videos (shared via MS Teams):
  - the duality between words and meanings (associated with the video): `v_2`
+ Python scripts/Jupyter notebooks used in the webinar:
  - `wb_2.ipynb`: Jupyter notebook used for the application 


Assignments
===========

+ `ps_1` is due by May 27  (8:00 PM)
+ `ps_1`: problem set due by June 2 (8:00 PM). Non mandatory. Send your solution to simone.santoni.1@city.ac.uk
