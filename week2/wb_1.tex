\begin{frame}{Week 2}
\protect\hypertarget{week-2}{}
\begin{itemize}
\tightlist
\item
  Representing words and meanings
\item
  Language modeling
\end{itemize}
\end{frame}

\begin{frame}{Agenda}
\protect\hypertarget{agenda}{}
\begin{itemize}
\tightlist
\item
  \href{}{}
\item
  \href{}{}
\item
  \href{}{}
\end{itemize}
\end{frame}

\begin{frame}{Tokenization}
\protect\hypertarget{tokenization}{}
Tokenization lies at the hearth of any natural language processing
analysis. Hence, it's.

\begin{block}{Naive tokenizer}
\protect\hypertarget{naive-tokenizer}{}
\end{block}

\begin{block}{NLTK tokenizers}
\protect\hypertarget{nltk-tokenizers}{}
\end{block}

\begin{block}{N-grams \& tokenizers}
\protect\hypertarget{n-grams-tokenizers}{}
\end{block}
\end{frame}

\begin{frame}{Modeling natural language: Main challenges}
\protect\hypertarget{modeling-natural-language-main-challenges}{}
Processing raw text intelligently is difficult:

\begin{itemize}
\tightlist
\item
  it's common for words that look completely different to mean almost
  the same thing
\item
  the same words in a different order can mean something completely
  different
\item
  most words are rare
\item
  even splitting text into useful word-like units can be difficult in
  many languages (see Japanese)
\end{itemize}

\emph{Source} is \href{https://spacy.io/usage/linguistic-features}{spaCy
website}

\begin{block}{Pre-DL models of the language}
\protect\hypertarget{pre-dl-models-of-the-language}{}
\end{block}

\begin{block}{Post-DL models of the language}
\protect\hypertarget{post-dl-models-of-the-language}{}
\end{block}
\end{frame}

\begin{frame}{The architecture of Natural Language Models}
\protect\hypertarget{the-architecture-of-natural-language-models}{}
\end{frame}
