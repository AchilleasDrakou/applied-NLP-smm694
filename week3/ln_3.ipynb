{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Week 3\n",
    "======\n",
    "\n",
    "## ― Vector semantics and embeddings\n",
    "\n",
    "<img src=\"images/_0.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Week 2: Key tenets\n",
    "================\n",
    "\n",
    "+ modern NLP ~ Distributional Hypothesis + DL\n",
    "+ words can be represented as vectors (Lenci 2018)\n",
    "+ desiderata: by observing and analyzing a same word in multiple context, we aim at:\n",
    "  - building a *dense, real-value* vector for each word\n",
    "  - ... chosen so that it is similar to vectors of words that appear in similar contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Word vectors as dense, real valued vectors\n",
    "===================================\n",
    "\n",
    "Ultimately, by observing and analyzing a same word in multiple context, we aim at building a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n",
    "\n",
    "Below is a portion of the [vector](https://spacy.io/usage/vectors-similarity) associated with the word 'banana'.\n",
    "\n",
    "```\n",
    "array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "       # ... and so on ...\n",
    "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why dense, real-valued vectors rather than sparse vectors?\n",
    "=================================================\n",
    "\n",
    "+ convenience: in ML, you don't want to deal with thousands of features \n",
    "+ they may do better at capturing synonymy/semantic similarity:\n",
    "  - 'car' and 'automobile' are synonyms; but are distinct dimensions\n",
    "  - a word with 'car' as a neighbor and a word with 'automobile' as a neighbor should be similar, but aren't\n",
    "+ in practice, they work better\n",
    "\n",
    "Notes. ― The materials included in the following slides are based on Jurafsky and Martin's book 'Speech and Language Processing and Speech Recognition' (chapter 6). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples of word-vectors and embeddings\n",
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[$\\texttt{word2vec}$](https://code.google.com/archive/p/word2vec/) ― the pioneer\n",
    "\n",
    "<img src=\"images/_1.png\" width=\"15%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[**Fasttext**](http://www.fasttext.cc/) ― it takes word parts into account\n",
    "\n",
    "<img src=\"images/_2.png\" width=\"6%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[**GloVe**](http://nlp.stanford.edu/projects/glove/) ― it emphasizes co-occurrences of words in the whole text corpus\n",
    "\n",
    "<img src=\"images/_3.png\" width=\"22%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's focus on $\\texttt{word2vec}$: Background\n",
    "=================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_4.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's focus on $\\texttt{word2vec}$: Background\n",
    "=================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_5.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{word2vec}$: the basic\n",
    "===================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key features**\n",
    "\n",
    "+ in 2013, it drew a line between old-school and modern NLP\n",
    "+ it doesn't require hand-labeled supervision\n",
    "+ easy and quite fast to train (you can do that with Gensim)\n",
    "+ it's OSS  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Philosophy**\n",
    "\n",
    "+ it trains a classifier on a binary prediction task:\n",
    "  - \"is word $\\omega$ likely to show up near word $\\eta$\"?\n",
    "+ the classification task is 'instrumental' in nature:\n",
    "  - the point is not predicting the 'next' word\n",
    "  - the goal is adjusting word vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{word2vec}$ algorithm: Skip-Gram flavor\n",
    "==================================\n",
    "\n",
    "**!!!Boundary condition¡¡¡**\n",
    "\n",
    "+ there are various flavors of the $\\texttt{word2vec}$\n",
    "+ here, we focus on the Skip-Gram (SG) flavor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Skip-Gram algorithm\n",
    "=================\n",
    "\n",
    "1. treat the target word and a neighboring context word as positive examples\n",
    "2. randomly sample other words in the lexicon to get negative samples\n",
    "3. use logistic regression to train a classifier to distinguish those two cases\n",
    "4. use the weights as the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training data\n",
    "==============\n",
    "\n",
    "Given the sentence:\n",
    "\n",
    "| not in context | c1         | c2 | t       | c3  | c4 | not in context | \n",
    "| -------------- |------------|----|---------|-----|----|----------------|\n",
    "| ... lemon, a   | tablespoon | of | apricot | jam | a  | pinch of ...   |\n",
    "\n",
    "let's assume context words are those in +/- 2 word window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG goal\n",
    "=======\n",
    " \n",
    "Given a tuple $(t,c)  = target, context$\n",
    "\n",
    "+ $\\texttt{(apricot, jam)}$\n",
    "+ $\\texttt{(apricot, aardvark)}$\n",
    "\n",
    "Return probability that $c$ is a real context word:\n",
    "\n",
    "$P(+|t,c)$\n",
    "\n",
    "$P(-|t,c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to compute p(+|t,c)?\n",
    "=====================\n",
    "\n",
    "Intuition:\n",
    "\n",
    "+ words are likely to appear near similar words\n",
    "+ model similarity with dot-product!\n",
    "+ similarity(t,c)  $∝ t ∙ c$\n",
    "\n",
    "Problem:\n",
    "+ dot product is not a probability!\n",
    "+ (neither is cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Turning dot product into a probability\n",
    "===============================\n",
    "\n",
    "\\begin{equation}\n",
    "P(+|t,c) = \\frac{1}{1+e^{-t ∙ c}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(-|t,c) = 1 - P(+|t,c)\n",
    "= \\frac{e^{-t ∙ c}}{1+e^{-t ∙ c}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For all context words\n",
    "=================\n",
    "\n",
    "\\begin{equation}\n",
    "P(+|t,c_{1:k}) = \\prod_{i=1}^{k} \\frac{1}{1+e^{-t ∙ c_{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training data\n",
    "=============\n",
    "Given the sentence:\n",
    "\n",
    "| not in context | c1         | c2 | t       | c3  | c4 | not in context | \n",
    "| -------------- |------------|----|---------|-----|----|----------------|\n",
    "| ... lemon, a   | tablespoon | of | apricot | jam | a  | pinch of ...   |\n",
    "\n",
    "let's assume context words are those in +/- 2 word window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training (1/2)\n",
    "============== \n",
    "\n",
    "Given the sentence:\n",
    "\n",
    "| not in context | c1         | c2 | t       | c3  | c4 | not in context | \n",
    "| -------------- |------------|----|---------|-----|----|----------------|\n",
    "| ... lemon, a   | tablespoon | of | apricot | jam | a  | pinch of ...   |\n",
    "\n",
    "we look for positive examples $+$:\n",
    "\n",
    "| t       | c          |\n",
    "|---------| -----------|\n",
    "| apricot | tablespoon |\n",
    "| apricot | of         |\n",
    "| apricot | jam        |\n",
    "| apricot | preserve   |\n",
    "| apricot | ...        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training (2/2)\n",
    "============== \n",
    "\n",
    "Given the list of + $c$:\n",
    "\n",
    "+ each positive $c$ is matched with a negative $c$\n",
    "+ negatives are 'noise words' that do not belong to any linguistic contexts of $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Setup\n",
    "=====\n",
    "\n",
    "Let's represent words as vectors of some length (say 300), randomly initialized. \n",
    "\n",
    "+ we start with 300 * V random parameters\n",
    "+ over the entire training set, we’d like to adjust those word vectors such that we\n",
    "+ maximize the similarity of the target word, context word pairs (t,c) drawn from the positive data\n",
    "+ minimize the similarity of the (t,c) pairs drawn from the negative data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Learning $\\texttt{word2vec}$ embeddings\n",
    "=============================\n",
    "\n",
    "<img src=\"images/_6.png\" width=\"80%\">\n",
    "\n",
    "Source is Jurafsky and Martin (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interpreting embeddings: Caveat\n",
    "===========================\n",
    "\n",
    "Semantic similarities depend on window size $C$:\n",
    "\n",
    "$C = ±2 \\rightarrow$ the nearest words to Hogwarts:\n",
    "+ sunnydale\n",
    "+ evernight\n",
    "\n",
    "$C = ±5 \\rightarrow$  the nearest words to Hogwarts:\n",
    "+ Dumbledore\n",
    "+ Malfoy\n",
    "+ halfblood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Embedding capture relational meanings\n",
    "================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "vector(‘king’) - vector(‘man’) + vector(‘woman’) = vector(‘queen’)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) = vector(‘Rome’)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_7.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Visual inspection of embeddings (1/2)\n",
    "===============================\n",
    "\n",
    "<img src=\"images/_8.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Visual inspection of embeddings (2/2)\n",
    "===============================\n",
    "\n",
    "<img src=\"images/_9.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Embeddings: Fields of application\n",
    "============================\n",
    "\n",
    "+ business and economic analysis (examples are scant)\n",
    "+ cultural analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Studying changes in meanings with Google Books data\n",
    "=============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/_10.png\" width=\"60%\">\n",
    "\n",
    "Source: see various works by Dr. W. Hamilton (McGill U.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/_11.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using embeddings as a historical tool to study bias\n",
    "==========================================\n",
    "\n",
    "<img src=\"images/_12.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using embeddings as a historical tool to study bias\n",
    "==========================================\n",
    "\n",
    "The paper in a nutshell:\n",
    "\n",
    "+ embeddings for competence adjectives are biased toward men\n",
    "  - smart, wise, brilliant, intelligent, resourceful, thoughtful, logical, etc.\n",
    "+ this bias is slowly decreasing over time"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
